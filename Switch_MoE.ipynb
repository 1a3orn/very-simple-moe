{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8187d4-b7a3-427b-bb8b-5826814e737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae4a9c-3658-41a7-aabe-9592c46cc8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import math\n",
    "import wandb\n",
    "import json\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.distributions as dist\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch import optim, nn, arange\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e474c4-f789-4fba-9cfe-8224e0f5cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6fa2b3-45ec-4578-851c-92a0379f053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# GENERIC TRANSFORMER PARTS\n",
    "# --------------------------------------------------\n",
    "#\n",
    "# Exactly the same in a MoE and a Dense transformer\n",
    "# - Causal Mask\n",
    "# - SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9b19d-3dcb-484b-a726-862c2cfd7867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla causal attention mask + hard-alibi\n",
    "# ------------------------------------------\n",
    "# Hard Alibi (https://arxiv.org/pdf/2402.01032.pdf) is a variant of\n",
    "# the Alibi position encoding (https://arxiv.org/pdf/2108.12409.pdf)\n",
    "# where Alibi's slow linear decay over the attention is replaced by\n",
    "# a discrete decay such that\n",
    "# - the first head sees one token back\n",
    "# - the second, two\n",
    "# - the third, four, and so on\n",
    "# ------\n",
    "# Init:\n",
    "# - config[\"max_length\"] = maximum length sequence fed into it\n",
    "# Forward:\n",
    "# - takes [b, t, t] or [b, nh, t, t]\n",
    "#\n",
    "class HardAlibiCausalMask(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = msl = config[\"max_length\"]\n",
    "        self.n_heads = n_heads = config[\"num_heads\"]\n",
    "        rel = self.get_relative_positions(msl)\n",
    "        stack = []\n",
    "        for i in range(n_heads):\n",
    "            bools = (rel <= 0) & (rel > -(2**i))\n",
    "            ready = torch.where(bools, torch.zeros(rel.shape), float(\"-inf\"))\n",
    "            stack.append(ready)\n",
    "        self.register_buffer(\"values\", torch.stack(stack, dim=0))\n",
    "\n",
    "    def get_relative_positions(self, seq_len):\n",
    "        x = torch.arange(seq_len)[None, :]\n",
    "        y = torch.arange(seq_len)[:, None]\n",
    "        return x - y\n",
    "\n",
    "    def forward(self, att):\n",
    "        sizes = att.size()\n",
    "        t1, t2 = sizes[-2:]\n",
    "        assert t1 == t2, \"attention must be square\"\n",
    "        assert t1 <= self.max_seq_len, \"attention must be smaller than max_seq_length\"\n",
    "        reshaped_att = att.view(-1, self.n_heads, t1, t1)\n",
    "        return reshaped_att + self.values[:,:t1,:t1]\n",
    "\n",
    "def test():\n",
    "    m = torch.rand(2, 4, 5, 5)\n",
    "    m = HardAlibiCausalMask({ \"max_length\": 6, \"num_heads\": 4, })(m)\n",
    "    # To get a feel for how hard-alibi works uncomment\n",
    "    #print(m)\n",
    "    assert m[0][0][0][0] > 0\n",
    "    assert m[0][0][1][1] > 0\n",
    "    assert m[0][0][0][1] == float(\"-inf\")\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737cb1c-caff-4958-84de-88b21f8162d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla multi-headed self-attention implementation\n",
    "# ------------------------------------------------\n",
    "# Init:\n",
    "# - config[\"hidden_dim\"] = int, dimension of input\n",
    "# - config[\"num_heads\"] = int, number of heads, head_size = hidden_dim // num_heads\n",
    "# - config[\"max_length\"] = int, sets masks size for the CausalMask\n",
    "# Forward:\n",
    "# - takes [b, t, c]\n",
    "# - outputs [b, t, c]\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hd = config[\"hidden_dim\"]\n",
    "        self.num_heads = nh = config[\"num_heads\"]\n",
    "        self.k = nn.Linear(hd, hd)\n",
    "        self.q = nn.Linear(hd, hd)\n",
    "        self.v = nn.Linear(hd, hd)\n",
    "        self.p = nn.Linear(hd, hd) # Projection from heads to output\n",
    "        self.mask = HardAlibiCausalMask(config)\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        b, t, c = x.size()\n",
    "\n",
    "        num_heads = self.num_heads\n",
    "        heads_dim = c // num_heads\n",
    "        k = self.k(x).view(b, t, num_heads, heads_dim).transpose(1, 2)\n",
    "        q = self.q(x).view(b, t, num_heads, heads_dim).transpose(1, 2)\n",
    "        v = self.v(x).view(b, t, num_heads, heads_dim).transpose(1, 2)\n",
    "\n",
    "        # Mult-headed attention: [b, nh, t, hd] @ [b, nh, hd, ] -> [b, nh, t, t]\n",
    "        att = q @ k.transpose(-2, -1)\n",
    "        att = att / math.sqrt(k.size(-1))\n",
    "        att = self.mask(att)\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "\n",
    "        # [b, nh, t, t] x [b, nh, t, hs] -> [b, nh, t, hs]\n",
    "        y = att @ v\n",
    "        \n",
    "        # [b, nh, t, hs] -> [b, t, nh, hs] -> [b, t, c]\n",
    "        y = y.transpose(1, 2).contiguous().view(b, t, c)\n",
    "        \n",
    "        return self.p(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80116b25-a9da-4fc0-b864-6878bb722f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# MLP vs MOE\n",
    "# --------------------------------------------------\n",
    "#\n",
    "# - MLP\n",
    "# - UnitCenteredNoise (used in SwitchMoE)\n",
    "# - SwitchMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632cbf7-fcc9-4a4a-81dc-2d0860683244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP module\n",
    "# -------------------------------\n",
    "# Note: Signature of init here matches init for MoE\n",
    "#\n",
    "# Init:\n",
    "# - config[\"hidden_dim\"] = dimension of c in [b, t, c] input\n",
    "# - index = int index\n",
    "# - scaling = amount by which to scale weights\n",
    "# Forward:\n",
    "# - tuple (tensor, aux_loss)\n",
    "# -- tensor is [b, t, c]\n",
    "# -- aux_loss is a scalar added to cross-entropy loss\n",
    "#\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config, index, scaling=1.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hd = config[\"hidden_dim\"]\n",
    "        self.seq = nn.Sequential(*[\n",
    "            nn.Linear(hd, hd * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hd * 4, hd),\n",
    "        ])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.seq[0].weight.data *= scaling\n",
    "            self.seq[2].weight.data *= scaling\n",
    "            self.seq[0].bias.data *= scaling\n",
    "            self.seq[2].bias.data *= scaling   \n",
    "\n",
    "    def forward(self, inp_tuple):\n",
    "        x, aux_loss = inp_tuple\n",
    "        return self.seq(x), aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf10b6-83ed-4460-8ad1-2cdf2295b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementwise multiplies: x * (1 +- eps)\n",
    "# -------------------------------\n",
    "# Init:\n",
    "# - scaling = amount by which 1 varies\n",
    "# Forward:\n",
    "# - any tensor\n",
    "#\n",
    "class UnitCenteredNoise(nn.Module):\n",
    "    def __init__(self, scaling=0.02):\n",
    "        super(UnitCenteredNoise, self).__init__()\n",
    "        self.scaling = scaling\n",
    "        self.base = 1 - (scaling * 0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # uniform 1-centered noise\n",
    "            noise = torch.rand(x.size()).to(x.device)\n",
    "            noise_centered = (noise * self.scaling) + self.base\n",
    "            return x * noise_centered\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b86872-9164-4541-ba08-5a788c6e9649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Switch Mixture-of-Experts Layer\n",
    "# -------------------------------\n",
    "# Note: Signature of init here matches init for MLP\n",
    "#\n",
    "# Init:\n",
    "# - config[\"hidden_dim\"] = Dimension of c in [b, t, c] input\n",
    "# - config[\"num_experts\"] = A layer indexed array of how many\n",
    "#                           experts are per layer\n",
    "# - config[\"init_moe_scaling\"] = Amount by which to scale\n",
    "#                                weights in MoE experts. If you don't\n",
    "#                                make them smaller, then it doesn't \n",
    "#                                learn as well for mysterious reasons.\n",
    "#                                \n",
    "# Forward:\n",
    "# - tuple (tensor, aux_loss)\n",
    "# -- tensor is [b, t, c]\n",
    "# -- aux_loss is a scalar added to cross-entropy loss\n",
    "class SwitchMoE(nn.Module):\n",
    "\n",
    "    def __init__(self, config, index):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hd = config[\"hidden_dim\"]\n",
    "        self.num_experts = num_experts = config[\"num_experts\"][index]\n",
    "        self.moe_scaling = moe_scaling = config[\"init_moe_scaling\"]\n",
    "        \n",
    "        self.experts = nn.ModuleList([\n",
    "            MLP(config, index=index, scaling=moe_scaling)\n",
    "            for index\n",
    "            in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(hd, num_experts),\n",
    "            UnitCenteredNoise(scaling=0.02),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, xx):\n",
    "        inp, aux_loss = xx\n",
    "        b, t, c = inp.shape\n",
    "\n",
    "        # Reshape to [b * t, c], makes it easier to think about\n",
    "        inp = inp.reshape(b * t, c)\n",
    "        \n",
    "        gate_val_continuous = self.gate(inp) # [b * t, c] -> [b * t, num_gates]\n",
    "        _, gate_val_indices = torch.topk(gate_val_continuous, 1, dim=-1) # [b * t, num_gates] -> [b * t, 1]\n",
    "        \n",
    "        # Map [b * t, 1] a one-hot [b * t, num_experts] where the last dim is one-hot encoded\n",
    "        one_hot = torch.nn.functional.one_hot(gate_val_indices, num_classes=self.num_experts).sum(1)\n",
    "\n",
    "        # Calculate auxillary loss to balance the experts\n",
    "        f = one_hot.sum(dim=0) # [b * t, num_experts] -> [num_experts]\n",
    "        f = f / f.sum()\n",
    "        P = gate_val_continuous.sum(dim=0) # [b * t, num_experts] -> [num_experts]\n",
    "        P = P / P.sum()\n",
    "        extra_aux_loss = (P * f).sum() * self.num_experts\n",
    "\n",
    "        output = torch.zeros_like(inp)\n",
    "        for i in range(self.num_experts):\n",
    "            \n",
    "            mask = one_hot[:,i] == 1 # mask shape: [b * t]\n",
    "            mask_expand = mask.unsqueeze(-1).expand_as(output) # to [b * t, c]\n",
    "      \n",
    "            inp_for_expert = inp[mask_expand].reshape(-1, c)\n",
    "            out_from_exp, _ = self.experts[i]((inp_for_expert, torch.zeros([1])))\n",
    "\n",
    "            output[mask_expand] =+ out_from_exp.reshape(-1)\n",
    "          \n",
    "        return output.reshape(b, t, c), extra_aux_loss + aux_loss\n",
    "        \n",
    "        \n",
    "def test():\n",
    "    a = torch.ones(7) / torch.ones(7).sum()\n",
    "    b = torch.ones(7) / torch.ones(7).sum()\n",
    "    print((a * b).sum())\n",
    "    \n",
    "    m = torch.randn(16, 32, 48).to(\"cuda\")\n",
    "    nm = SwitchMoE({\"hidden_dim\": 48, \"num_experts\": [8,8,8], \"init_moe_scaling\": 1.0}, 1).to(\"cuda\")\n",
    "    out = nm((m, torch.zeros([1]).to(\"cuda\")))\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9794ea-7d63-42d4-a18b-75e965369cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# STANDARD TRANSFORMER: WHOLE THING\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c28189-3b13-4d43-a25e-a92b150d4ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config, index):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hd = config[\"hidden_dim\"]\n",
    "        self.attention = SelfAttention(config)\n",
    "\n",
    "        # Switch between a MoE and a MLP layer according to config\n",
    "        if config[\"layer_types\"][index] == \"switch\":\n",
    "            MLPClass = SwitchMoE\n",
    "        elif config[\"layer_types\"][index] == \"mlp\":\n",
    "            MLPClass = MLP\n",
    "        else:\n",
    "            raise Exception(\"Invalid layer type\")\n",
    "        \n",
    "        self.ff = MLPClass(config, index)\n",
    "        self.norm1 = nn.LayerNorm([hd])\n",
    "        self.norm2 = nn.LayerNorm([hd])\n",
    "\n",
    "    def forward(self, input_tuple):\n",
    "        x, extra_loss = input_tuple\n",
    "\n",
    "        x_att = self.attention(x)\n",
    "        x = x + self.norm1(x_att)\n",
    "\n",
    "        x_ff, extra_loss = self.ff((x, extra_loss))\n",
    "        x = x + self.norm2(x_ff)\n",
    "        \n",
    "        return x, extra_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e60cc-04fa-4112-998c-575296ab2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.input_dim = self.output_dim = config[\"input_dim\"]\n",
    "        self.hidden_dim = hd = config[\"hidden_dim\"]\n",
    "        self.max_length = config[\"max_length\"]\n",
    "        self.layer_types = config[\"layer_types\"]\n",
    "        self.layer_num = layer_num = len(config[\"layer_types\"])\n",
    "        self.num_experts = config[\"num_experts\"] # note this is an array\n",
    "        \n",
    "        self.char_embed = nn.Embedding(self.input_dim, hd)\n",
    "\n",
    "        self.stack = nn.Sequential(*[\n",
    "            TransformerBlock(config, x) for x in range(layer_num)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(hd)\n",
    "        self.to_prob = nn.Linear(hd, self.output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.size()\n",
    "        # Initial input proc: char_embed\n",
    "        inp = self.char_embed(x)\n",
    "\n",
    "        # Initialize aux_loss with 0\n",
    "        aux_loss = torch.tensor(0)\n",
    "        x, aux_loss = self.stack((inp, aux_loss))\n",
    "\n",
    "        x = self.layer_norm(x.view(b * t, self.hidden_dim))\n",
    "        \n",
    "        return self.to_prob(x).view(b, t, self.output_dim), aux_loss\n",
    "\n",
    "    def get_param_groups(self, base_learning_rate):\n",
    "        # Scale the learning rate for each expert by 1 / sqrt(num_experts)\n",
    "        blr = base_learning_rate\n",
    "        param_groups = []\n",
    "        for name, param in self.named_parameters():\n",
    "            \n",
    "            if \"stack\" in name and \"ff.experts\" in name:\n",
    "                for i in range(self.layer_num):\n",
    "                    to_find = f\"stack.{i}.ff.experts\"\n",
    "                    if to_find in name:\n",
    "                        per_expert_lr = blr / math.sqrt(self.num_experts[i])\n",
    "                        param_groups.append({ \"params\": param, \"lr\": per_expert_lr })\n",
    "                        break\n",
    "            else:\n",
    "                param_groups.append({\"params\": param, \"lr\": blr })\n",
    "\n",
    "        return param_groups\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e32920-a92b-4c77-8eb3-8d70747c108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warms up to learning rate over 'steps_up'\n",
    "# Cools down to LR * gamma over 'steps_down'\n",
    "class LinearLRDecayWithWarmup(_LRScheduler):\n",
    "    def __init__(self, optimizer, steps_up, steps_down, gamma, last_epoch=-1):\n",
    "        self.steps_up = steps_up\n",
    "        self.steps_down = steps_down\n",
    "        self.gamma = gamma\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "        \n",
    "    def get_lr(self):\n",
    "\n",
    "        if self.last_epoch > self.steps_down + self.steps_up:\n",
    "            return [base_lr * self.gamma for base_lr in self.base_lrs]\n",
    "\n",
    "        if self.last_epoch > self.steps_up:\n",
    "            steps_after_up = self.last_epoch - self.steps_up\n",
    "            percentage_there = steps_after_up / self.steps_down\n",
    "            mult = (1 - percentage_there) * 1 + (percentage_there * self.gamma)\n",
    "            return [base_lr * mult for base_lr in self.base_lrs]\n",
    "            \n",
    "        return [base_lr * (self.last_epoch / self.steps_up) for base_lr in self.base_lrs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82091748-8c4f-440c-97c6-de8ad194903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a text file, tokenizes it extreeemely simply,\n",
    "# and just shoves it into memory as a PyTorch Tensor\n",
    "#\n",
    "# I've been using a ~200 mb scrape of Gutenberg\n",
    "# books to test, but anything of similar size should\n",
    "# work fine\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_name, ctx_len):\n",
    "        self.ctx_len = ctx_len\n",
    "        \n",
    "        # Load and preprocess the text\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        print(\"Read file \", file_name)\n",
    "\n",
    "        \n",
    "        # Filter out non-string characters\n",
    "        text = ''.join(filter(lambda x: x in string.printable, text))\n",
    "        print(\"Filtered file \")\n",
    "        length = len(text)\n",
    "        \n",
    "        # Tokenize by character\n",
    "        tokens = [string.printable.index(c) for c in text]\n",
    "        print(\"Tokenized file\")\n",
    "\n",
    "        # Create chunks of ctx_len\n",
    "        self.data = []\n",
    "        i = 0\n",
    "        while((i + 1) * ctx_len < length):\n",
    "            peeled = tokens[i * ctx_len:(i + 1) * ctx_len]\n",
    "            self.data.append(peeled)\n",
    "            if (len(self.data)) % 100000 == 0:\n",
    "                print(len(self.data), len(peeled), len(tokens))\n",
    "            i = i + 1\n",
    "        print(\"Chunked\", len(self.data), len(self.data[0]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebbbea-501f-4b1a-9df4-a37ee3e3485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_KEYS = [\n",
    "    # MODEL DETAILS\n",
    "    #\n",
    "    # List of either 'mlp' or 'switch_moe'\n",
    "    \"layer_types\",\n",
    "    # List of ints, for how many experts go in IF it is a moe -- does nothing otherwise\n",
    "    \"num_experts\",\n",
    "    # Maximum length of input\n",
    "    \"max_length\",\n",
    "    \"hidden_dim\",\n",
    "    # How many input and output classes we have\n",
    "    \"input_dim\",\n",
    "    \"num_heads\",\n",
    "    # Scaling factor for MoE expert weights\n",
    "    \"init_moe_scaling\",\n",
    "\n",
    "    # TRAINING DETAILS\n",
    "    #\n",
    "    # Learning rate\n",
    "    \"lr\",\n",
    "    # How long to warm up from 0 lr\n",
    "    \"lr_steps_up\",\n",
    "    # How long to cool down from lr to lr * gamma\n",
    "    \"lr_steps_down\",\n",
    "    # Final learning rate = lr * gamma\n",
    "    \"lr_gamma\", \n",
    "    \"aux_loss_weight\",\n",
    "    # Text file to go through epochs times\n",
    "   \n",
    "    \"file\",\n",
    "    \"epochs\",\n",
    "    \"batch_size\",\n",
    "    \"log_interval\",\n",
    "    \"device\",\n",
    "]\n",
    "\n",
    "def config_to_run_name(config):\n",
    "    return \"__\".join([\n",
    "        \"hd_\" + str(config[\"hidden_dim\"]),\n",
    "        \"layers_\" + str(len(config[\"layer_types\"])),\n",
    "        \"lr_\" + str(config[\"lr\"]),\n",
    "        \"file_\" + str(config[\"file\"]),\n",
    "    ])\n",
    "\n",
    "def verify_config(config):\n",
    "    keys_mandatory = set(CONFIG_KEYS)\n",
    "    keys_used = set(config.keys())\n",
    "    keys_dif = keys_mandatory ^ keys_used\n",
    "    assert len(keys_dif) == 0, f\"Unrecognized or required keys: {keys_dif}\"\n",
    "    assert len(config[\"layer_types\"]) == len(config[\"num_experts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1e0a3-cd56-4191-a704-4f6db29516e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_mlp_layer(config):\n",
    "\n",
    "    verify_config(config)\n",
    "    model = Transformer(config).to(config[\"device\"])\n",
    "    \n",
    "    config[\"total_params\"] = tp = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total Model Parameters: {(tp / 1000000.0):.2f}m\")\n",
    "    print(\"Config: \", json.dumps(config, indent=4))\n",
    "    \n",
    "    # Load data\n",
    "    dataset = TextDataset(config[\"file\"], ctx_len=config[\"max_length\"] + 1)\n",
    "    dataloader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    # Define the loss function, optimizer, param groups\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.get_param_groups(config[\"lr\"]))\n",
    "    scheduler = LinearLRDecayWithWarmup(\n",
    "        optimizer,\n",
    "        steps_up=config[\"lr_steps_up\"],\n",
    "        steps_down=config[\"lr_steps_down\"],\n",
    "        gamma=config[\"lr_gamma\"]\n",
    "    )\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"switch-transformer-hard-alibi\",\n",
    "        config=config,\n",
    "        name=config_to_run_name(config)\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            inputs = batch[:, :-1].to(config[\"device\"])\n",
    "            targets = batch[:, 1:].to(config[\"device\"])\n",
    "    \n",
    "            outputs, loss_kl = model(inputs)\n",
    "            loss_ce = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            loss = loss_ce + (loss_kl * config[\"aux_loss_weight\"])    \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            wandb.log({\n",
    "                \"loss\": loss,\n",
    "                \"loss_normal\": loss_ce,\n",
    "                \"loss_kl\": loss_kl,\n",
    "                \"lr\": scheduler.get_last_lr()[0]\n",
    "            })\n",
    "\n",
    "            # We only care about KL loss instrumentally\n",
    "            loss_sum += loss_ce.item()\n",
    "            if i % config[\"log_interval\"] == 0:\n",
    "                print(i, \", loss: \", loss_sum / config[\"log_interval\"])\n",
    "                loss_sum = 0\n",
    "    \n",
    "        print(f\"Epoch {epoch+1} Done\")\n",
    "        \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79f5f5-3900-46f1-bdc6-a635016b12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a simple dense Transformer with ~20m parameters\n",
    "run_with_mlp_layer({\n",
    "    \"layer_types\": [\"mlp\",\"mlp\",\"mlp\",\"mlp\",\"mlp\",\"mlp\",\"mlp\"],\n",
    "    \"num_experts\": [None, None, None, None, None, None, None],\n",
    "    \"max_length\": 512,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"num_heads\": 8,\n",
    "    \"input_dim\": len(string.printable),\n",
    "    \"lr_steps_down\": 4500,\n",
    "    \"lr_steps_up\": 500,\n",
    "    \"lr_gamma\": 0.1,\n",
    "    \"lr\": 0.0004,\n",
    "    \"aux_loss_weight\": 0.08,\n",
    "    \"init_moe_scaling\": 0.0625,\n",
    "    \"file\": \"two.txt\",\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 64,\n",
    "    \"log_interval\": 500,\n",
    "    \"device\": \"cuda\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e235e-9ace-429b-a87e-7de68c7e70bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a MoE Transformer with ~75m parameters\n",
    "run_with_mlp_layer({\n",
    "    \"layer_types\": [\"mlp\",\"mlp\",\"mlp\",\"switch\",\"mlp\",\"switch\",\"mlp\"],\n",
    "    \"num_experts\": [None, None, None, 12, None, 12, None],\n",
    "    \"max_length\": 512,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"num_heads\": 8,\n",
    "    \"input_dim\": len(string.printable),\n",
    "    \"lr_steps_down\": 4500,\n",
    "    \"lr_steps_up\": 500,\n",
    "    \"lr_gamma\": 0.1,\n",
    "    \"lr\": 0.0004,\n",
    "    \"aux_loss_weight\": 0.06,\n",
    "    \"init_moe_scaling\": 0.0625,\n",
    "    \"file\": \"two.txt\",\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 64,\n",
    "    \"log_interval\": 500,\n",
    "    \"device\": \"cuda\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468b32f-a018-40f9-81d7-13c80e70a9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
